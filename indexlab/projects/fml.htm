<!doctype html>
<html lang="en">
<head>
	<!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">
	<link rel="icon" href="imgs/InDeXLab.gif"/>
    <title>InDeX Lab. AI&ML</title>
    <link rel="stylesheet" type="text/css" href="../ppl.css" />
    <link rel="stylesheet" type="text/css" href="../style.css" />
    <script type="text/javascript" src="../js/myobjects.js"></script>
    <script type="text/javascript" src="../js/MyFunctions.js"></script>
    <script type="text/javascript" src="../js/myobjects.js"></script>
    <script type="text/javascript" src="../js/indexstart.js"></script>
    <script type="text/javascript">
        function init() {
            header('../');
        }
    </script>
</head>

<body onload='init()'>
    <div id="headerDiv"></div>
    <!-- Your code starts here -->
    <div class="container">
        <div class="text-left">
          <ul>
                      <li><a href="#FairR">OmniFair</a></li>
                      <li><a href="#StableR">Fair Active Learning</a></li>
          </ul>
         </div>
  
         <div>&nbsp;</div>
         <div id="FairR"><div class="card">
  <div class="card-header">Fair Ranking Schemes</div>
  <div class="card-body">
  <ul class="nav nav-tabs" id="FairR" role="tablist">
      <li class="nav-item"><a class="nav-link active" id="FRA-tab" data-toggle="tab" href="#FRAbstract" role="tab" aria-controls="home" aria-selected="true">Abstract</a></li>
      <li class="nav-item"><a class="nav-link" id="FRI-tab" data-toggle="tab" href="#FRIntro" role="tab" aria-controls="profile" aria-selected="false">Extended Description</a></li>
      <li class="nav-item"><a class="nav-link" id="FRP-tab" data-toggle="tab" href="#FRPubs" role="tab" aria-controls="contact" aria-selected="false">Publications</a></li>
      <li class="nav-item"><a class="nav-link" id="FRC-tab" data-toggle="tab" href="#FRCol" role="tab" aria-controls="contact" aria-selected="false">Collaborators</a></li>
  </ul>
  <div class="tab-content" id="FRContent">
      <div class="tab-pane active" id="FRAbstract" role="tabpanel" aria-labelledby="FRA-tab">
          Items from a database are often ranked based on a combination
          of criteria. The weight given to each criterion in the
          combination can greatly affect the fairness of the produced
          ranking, for example, preferring men over women. A user
          may have the flexibility to choose combinations that weigh
          these criteria differently, within limits. In this project, we develop
          a system that helps users choose criterion weights
          that lead to greater fairness.
      </div>
      <div class="tab-pane " id="FRIntro" role="tabpanel" aria-labelledby="FRI-tab">
              <p>
                      Ranking of individuals is commonplace today, and is used, for example, to establish credit worthiness, desirability for college admissions and employment, and attractiveness as dating partners. 
                      A prominent family of ranking schemes are score-based rankers, which compute the score of each individual from some database D, sort the individuals in decreasing order of score, and finally return either the full ranked list, or its highest-scoring sub-set, the top-k.  Many score-based rankers compute the score as a linear combination of attribute values, with non-negative weights.  
                  </p>
                  <p>
                          This sort of linear-weighted scoring and ranking is ubiquitous.  Many sports use such schemes.  For example, tennis players have an ATP rank based on a score that weights each level of success (winner, finalist, semi-finalist, and so on) at each type of tournament, and adds these up.  A score with more serious implications is the credit score that each person has in many countries, meant to indicate creditworthiness. Even in the context of academic research, we see such scoring: many funding agencies compute a score for a research proposal as a weighted sum of scores for its attributes.
                  <!--</p><p>
                          Because of the potential impact of such rankings on individuals and on population groups, issues of algorithmic bias and discrimination are coming to the forefront of societal and technological discourse~\cite{BarocasSelbst}.  In their seminal work Friedman and Nissenbaum~\cite{DBLP:journals/tois/FriedmanN96} define a biased computer system as one that (1) systematically and unfairly discriminates against some individuals or groups in favor of others, and (2) joins this discrimination with an unfair outcome.
                  -->
                  
                  </p><p>
                      We desire a ranking scheme that is fair, in the sense that it mitigates <i>preexisting bias with respect to a protected feature</i> embodied in the data. 
                      In line with prior work, a protected feature denotes membership of an individual in a legally-protected category, such as persons with disabilities, or under-represented minorities by gender or ethnicity.  
                      We refer to such categories (e.g., minority ethnicity) as <i>protected groups</i>, and to the attributes that define them (e.g., ethnicity) as <i>sensitive attributes</i>. 
                      Numerous fairness definitions have been considered in the recent literature.
                      A useful dichotomy is between <i>individual fairness</i> and <i>group fairness</i>, also known as statistical parity. The former requires  that similar individuals be treated similarly, while the latter requires that demographics of those receiving a particular outcome are identical or similar to the demographics of the population as a whole.
                      These two requirements represent intrinsically different world views, and accommodating both requires trade-offs.
                      Our focus in this peoject is on group fairness.
                      <!--While our techniques apply to a broad range of group fairness criteria, to make our discussion concrete we will define fairness in terms of minimum bounds on the number of selected members of a protected group at the top-k, for some reasonable value of $k$~\cite{DBLP:conf/icalp/CelisSV18}.-->
  
                  </p>
                  <p class="text-center"><img src="assets/fair.jpg"> </p>
                  <p>
                          Designing a ranking scheme amounts to selecting a set of weights, one for each attribute.
                          In many situations, such as to rank tennis players, research proposals, or academic departments, we do not have access to any ground truth and resort to subjectively selected weights in a simple additive scoring function.
                          Such a function is typically defined over a handful of attributes, due to 
                          the cognitive burden of selecting the scoring criteria and coming up with an appropriate weight vector.
                          The question we address in this project is how to introduce fairness into this subjective weight selection process.
                          Consider an example.    
                  </p>
                  <div class="card">
                      <div class="card-header text-center">EXAMPLE</div>
                      <div class="card-body">
                          A college admissions officer is designing a ranking scheme to evaluate a pool of applicants, each with several potentially relevant attributes.
                          For simplicity, let us focus on two of these attributes --- high school GPA and SAT score. Suppose that our fairness criterion is that the admitted class comprise at least 40% women.
                          As the first step, to make the score components comparable, GPA and SAT scores may be normalized and standardized.  We will denote the resulting values <code>g</code> for GPA and <code>s</code> for SAT.  
                          The admissions officer may believe a priori that <code>g</code> and <code>s</code> should have an approximately equal weight, computing the score of an applicant
                          <code>t ∈ D</code> as <code>f(t) = 0.5 × s + 0.5 × g</code>
                          , ranking the applicants, and returning the top 500 individuals.
                          Upon inspection, it may be determined that an insufficient number of women is returned among the top-k: at least 200 women were expected to be among the top-500, and only 150 were returned, violating our fairness constraint. This violation may be due to a gender disparity in the data: in 2014, women scored about 25 points lower on average than men in the SAT test.  
                          Note that the admissions officer was not looking at the sensitive attribute (gender, in our example), and proposed a scoring function that is not obviously biased against women: the lack of fairness is only observed in the outcome.
                          Our goal in this project is to build a system that will assist the admissions officer in identifying alternative scoring functions that meet the fairness constraint and are close to the original function <code>f</code> in terms of attribute weights, thereby reflecting the admission officer's a priori notion of quality.  
                          After a few cycles of such interaction with the system, the admissions officer may choose <code>f'(t) = 0.45 × s + 0.55 × g</code> as the final scoring function.
                      </div>
                  </div>
                  <p>
                          As underscored by the above Example, we wish to produce results that are both fair ---  as stated by the fairness constraints, and of high quality --- as stated by the initial scoring function weights.
                          These initial  scoring function weights will only approximate quality, for two reasons.
                          First, observational data usually contains imperfect proxies of "true" aspects of quality (e.g., SAT score vs. intelligence, and GPA vs. grit).
                          Second, future outcomes cannot be perfectly predicted based on present observations, irrespective of whether a simple score-based rankers or a complex learned model is used.
                  </p>
                  <p>
                          Rather than adjusting the scoring function, one could meet fairness constraints by having different cutoff scores for different demographic groups.  For example, the admissions officer in the above Example could have stuck with the original function, and used a lower score threshold for admitting women compared to the one for men.
                          While such a fix is technically easy, it is illegal in many jurisdictions, because it amounts to <span class="bg-warning">disparate treatment</span> --- to the explicit use of a protected characteristic such as gender or race to make decisions. Our proposal in this project navigates the trade-off between disparate treatment
                          and <span class="bg-warning">disparate impact</span> ---  providing outputs that hurt members of a protected group more frequently than members of other groups.
                          If a small adjustment to the weights of the scoring function can achieve fairness, that may be both preferable for legal reasons, and acceptable from the point of view of utility, particularly since the original weights likely were approximate values chosen subjectively.
                  </p>
                  <p>
                          Of course, our proposed methods will not prevent institutional racism and other kinds of intentional discrimination.
                          That said, it is increasingly recognized that this challenge cannot be addressed by technology alone, and that responsibility to determine the context of use of a tool should fall  squarely on legal and policy frameworks.  Rather than dictating a particular choice of policy, we enable decision makers to <i> transparently enact a policy</i> of their choosing, by supporting an explicit specification of fairness constraints, and incorporating them into ranking scheme design. Mechanisms that embody <i> transparency by design </i> are now required by legal and policy frameworks, such as the New York City algorithmic transparency law (Local Law 1696-2017).
                  </p>
                  <p>
                          Our technical goal is to build <span class="bg-success">a system that assists a human designer of a scoring function in tuning attribute weights to achieve fairness</span>.  Since this tuning process does not occur too often, it may be acceptable for it to take some time. However, we know that humans are able to produce superior results when they get quick feedback in a design or analysis loop.  Indeed, it is precisely this need that is a central motivation for OLAP, rather than having only long-running analytics queries. Ideally, a  designer of a ranking scheme would want the system to support her work through interactive response times.  Our goal is to meet this need, to the extent possible.
                  </p>
                  <p>
                          Unfortunately, it is computationally challenging to find a scoring function that is both fair and close to the user-specified scoring function, particularly when more than two scoring attributes must be considered.
                          In order to over come this challenge, we introduce techniques from combinatorial geometry and, since the direct application of the existing algorithm does not scale in practice, we propose the  arrangement tree data structure. We then propose a grid-partitioning preprocessing that enables approximate query answering.
                  </p>
                  <p>
                          In addition to the offline preprocessing method, we also study sampling for on-the-fly query answering.
                          We provide a negative result that the methods based on function sampling, cannot provide any guarantees for the discovery of an approximate solution.
                  </p>
      </div>
      <div class="tab-pane " id="FRPubs" role="tabpanel" aria-labelledby="FRP-tab">
          <ul>
              <li>Abolfazl Asudeh, H.V. Jagadish, Julia Stoyanovich, Gautam Das. <a target="_blank" href="https://dl.acm.org/citation.cfm?id=3300079">Designing Fair Ranking Schemes</a>. <i>SIGMOD</i>, 2019, ACM.</li>
              <li>(Invited Paper) Abolfazl Asudeh, H.V. Jagadish, Julia Stoyanovich. <a target="_blank" href="http://sites.computer.org/debull/A19sept/issue1.htm">Towards Responsible Data-driven Decision Making in Score-Based Systems</a>. Data Engineering Bulletin, Vol. 42(3), pages 76–87, 2019, Special Issue on Fairness, Diversity, and Transparency in Data Systems.</li>
          </ul>
      </div>
      <div class="tab-pane " id="FRCol" role="tabpanel" aria-labelledby="FRC-tab">
          <ul>
              <li>H.V. Jagadish, University of Michigan</li>
              <li>Julia Stoyanovich, New York University</li>
              <li>Gautam Das, University of Texas at Arlington</li>
          </ul>
      </div>
  </div>
  </div><!-- class="card" --></div></div>
         <div>&nbsp;</div><div id="StableR"><div class="card">
  <div class="card-header">Fair Active Learning</div>
  <div class="card-body">
  <ul class="nav nav-tabs" id="StableR" role="tablist">
      <li class="nav-item"><a class="nav-link active" id="SRA-tab" data-toggle="tab" href="#SRAbstract" role="tab" aria-controls="home" aria-selected="true">Abstract</a></li>
      <li class="nav-item"><a class="nav-link" id="SRI-tab" data-toggle="tab" href="#SRIntro" role="tab" aria-controls="profile" aria-selected="false">Extended Description</a></li>
      <li class="nav-item"><a class="nav-link" id="SRP-tab" data-toggle="tab" href="#SRPubs" role="tab" aria-controls="contact" aria-selected="false">Publications</a></li>
      <li class="nav-item"><a class="nav-link" id="SRC-tab" data-toggle="tab" href="#SRCol" role="tab" aria-controls="contact" aria-selected="false">Collaborators</a></li>
  </ul>
  <div class="tab-content" id="SRContent">
      <div class="tab-pane active" id="SRAbstract" role="tabpanel" aria-labelledby="SRA-tab">
          Decision making is challenging when there is more than one crite-
          rion to consider. In such cases, it is common to assign a goodness
          score to each item as a weighted sum of its attribute values and
          rank them accordingly. Clearly, the ranking obtained depends on
          the weights used for this summation. Ideally, one would want the
          ranked order not to change if the weights are changed slightly. We
          call this property stability of the ranking. A consumer of a ranked
          list may trust the ranking more if it has high stability. A producer of
          a ranked list prefers to choose weights that result in a stable rank-
          ing, both to earn the trust of potential consumers and because a
          stable ranking is intrinsically likely to be more meaningful.
          In this paper, we develop a framework that can be used to assess
          the stability of a provided ranking and to obtain a stable ranking
          within an “acceptable” range of weight values (called “the region
          of interest”). We address the case where the user cares about the
          rank order of the entire set of items, and also the case where the user
          cares only about the top-k items. Using a geometric interpretation,
          we propose algorithms that produce stable rankings. In addition
          to theoretical analyses, we conduct extensive experiments on real
          datasets that validate our proposal.
      </div>
      <div class="tab-pane " id="SRIntro" role="tabpanel" aria-labelledby="SRI-tab">
          <p>
                  <img src="assets/stability1.jpg" class="rounded float-right" style="padding-left: 10px;">
                  It is often useful to rank items in a dataset. It is straightforward to sort on a single attribute, but that is often not enough. When the items have more than one attribute on which they can be compared, it is challenging to place them in ranked order.
                  Consider, for example, the problem of ranking computer science departments.  Various entities, such as U.S. News and World Report, Times Higher Education, and the National Research Council, produce such rankings.  
                  These rankings are impactful, yet heavily criticized.
                  Several of these rankings have deficiencies in the attributes they choose to measure and in their data collection methodology, not of relevance to our paper now.
                  Our concern is that even if these deficiencies were addressed, we are compelled to obtain a single score/rank for a department by combining multiple objective measures, such as publications, citations, funding, and awards.  Different ways of combining values for these attributes can lead to very different rankings.  There are similar problems when we want to rank/seed sports teams, rank order cars or other products, 
                  <a href="https://www.newyorker.com/magazine/2011/02/14/the-order-of-things" target="_blank">as Malcolm Gladwell has nicely described</a>.
          </p>
          <p>
                  Differences in rank order can have significant consequences. 
                  For example, a company may promote high-ranked employees and fire low-ranked employees.
                  In university rankings, it is well-documented that the ranking formula has a significant effect on policies adopted by universities. In other words, it matters how we choose to combine values of multiple attributes into a scoring formula.
                  Even when there is lack of consensus on a specific way to combine attributes, we should make sure that the method we use is robust: it should not be the case that small perturbations, such as small changes in parameter values, can change the rank order.
          </p>
          <p>
                  In this project we address the following problem: <span class="bg-info text-warning">Assume that a linear combination of the attribute values is used for assigning a score to each item; then items are sorted to produce a final ranked order.  We want this ranking to be <i>stable</i> with respect to changes in the weights used in scoring</span>.  Given a particular ranked list of items, one question a consumer will ask is: how robust is the ranking?   If small changes in weights can change the ranked order, then there cannot be much confidence in the correctness of the ranking. 
          </p>
          <p>
                  A given ranking of a set of items can be generated by many weight functions.  Because this set of functions is continuous, we can think of it as forming a region in the space of all possible weight functions.  We call a ranking of items stable if it is generated by a weight function that corresponds to a large region of this space.
          </p>
          <p>
                  Stability is a natural concern for consumers of a ranked list (i.e. those who use the ranking to prioritize items and make decisions), who should be able to assess the magnitude of the region in the weight space that produces the observed ranking.  If this region is large, then the same ranked order would be obtained for many choices of weights, and the ranking is stable.  But if this region is small, then we know that only a few weight choices can produce the observed ranking.
                  This may suggest that the ranking was engineered or <span class="bg-warning">cherry-picked</span> by the producer to obtain a specific outcome. 
          </p>
          <p>
                  Data scientists often act as producers of ranked lists (i.e. they design weight functions that result in ranked lists), and desire to produce stable results.  Stability in a ranked output is an important aspect of algorithmic transparency, because it allows the producer to justify their ranking methodology, and to gain the trust of consumers.
                  Of course, stability cannot be the only criterion in the choice of a ranking function: the result may be weights that seem ``unreasonable'' to the ranking producer.  To support the producer, we allow them to specify a range of reasonable weights, or an <i>acceptable region</i> in the space of functions, and help the producer find stable rankings within this region. 
          </p>
          <p>
                  Our work is motivated by the lack of formal understanding of ranking stability and the consequent lack of tools for consumers and producers to assess this critical property of rankings.  We will show that stability hinges on complex geometric properties of rankings and weight functions.  We will provide a novel technique to identify stable rankings efficiently.
                  Our technique does not stop at proposing just the single most stable choice, or even the <i>h</i> most stable choices for some pre-determined fixed value of <i>h</i>.
                  Rather, it will continue to propose weight choices, and the corresponding rank ordering of items, beginning with the most stable in the specified region of interest, and continuing in decreasing order of stability, until the user finds one that is satisfactory.
                  Alternatively, our technique can provide an overview of all the rankings that occupy a large portion in the acceptable region, and hence are stable, along with an indication of the fraction of the acceptable region occupied by each.  Thereby, the user can see at a glance what the stable options are, and also how dominant these are within the acceptable region.
                  Let us motivate our techniques with an example:
          </p>
          <div class="card">
                  <div class="card-header text-center">EXAMPLE</div>
                  <div class="card-body">
                          <a href="http://csmetrics.org/" target="_blank">CSMetrics</a> ranks computer science research institutions based on the measured (<code>M</code>) and predicted (<code>P</code>) number of citations.
                          These values are appropriately scaled and used in a weighted scoring formula, with parameter <code>α ∈ [0,1]</code> that sets their relative importance (more details in the paper). 
                          CSMetrics includes a handful of companies in its ranking, but we focus on academic departments in this example.
                          As <code>α</code> ranges from 0 to 1, CSMetrics generates 336 distinct rankings of the top-100 departments.  Assuming (as a baseline) that each ranking is equally likely, we would expect an arbitrarily chosen ranking to occur 0.3% of the time, which we take to mean that it occupies 0.3% of the volume in the space of attributes and weights.
                          We formalize this as <i>stability of a ranking</i>.
                          <br>
                          Suppose that the ranking with <code>α=0.3</code> is released, placing Cornell (a consumer) at rank 11, just missing the top-10.
                          Cornell then checks the stability of the ranking and learns that it's value is 0.3%, matching that of the uniform baseline.  With this finding, Cornell asks CSMetrics to justify its choice of <code>α</code>.
                          <br>
                          CSMetrics (the producer) can respond to Cornell by further interrogating, and potentially revising the published ranking.
                          It first enumerates stable regions and finds that the most stable ranking indeed places Cornell at rank 10 (switching with the University of Toronto), and represents 2% of the volume --- an order of magnitude more than the reference ranking.
                          However, this stable ranking is very far from the default, placing more emphasis on measured citations with <code>α=0.608</code>.
                          If this is unsatisfactory, CSRankings can propose another ranking closer to the reference ranking, but with better stability.
                          Interestingly, Cornell also appears at the top-10 in the most stable ranking that is within 0.998 cosine similarity from the original scoring function. 
                  </div>
          </div>
              <p>
                  Besides studying the efficiency of our techniques, our empirical evaluation investigates the stability of real published rankings of computer science departments, soccer teams, and diamond retailers. 
                  We show that existing rankings in these domains are often unstable and that favoring stability can sometimes have a significant impact on the rank of some items.
                  For instance, <span class="bg-warning">our findings cast doubt on the validity of <a href="https://www.fifa.com/fifa-world-ranking/ranking-table/men/" target="_blank"> the FIFA rankings</a>
                  which are used in making important decisions such as seeding the World Cup final draws.</span>
              </p>	
      </div>
      <div class="tab-pane " id="SRPubs" role="tabpanel" aria-labelledby="SRP-tab">
          <ul>
              <li>Abolfazl Asudeh, HV Jagadish, Gerome Miklau, Julia Stoyanovich. <a target="_blank" href="https://dl.acm.org/citation.cfm?id=3308984">On obtaining stable rankings</a>. PVLDB, Vol. 12(3), pages 237--250, 2019, VLDB Endowment.</li>
              <li>(Invited Paper) Abolfazl Asudeh, H.V. Jagadish, Julia Stoyanovich. <a target="_blank" href="http://sites.computer.org/debull/A19sept/issue1.htm">Towards Responsible Data-driven Decision Making in Score-Based Systems</a>. Data Engineering Bulletin, Vol. 42(3), pages 76–87, 2019, Special Issue on Fairness, Diversity, and Transparency in Data Systems.</li>
          </ul>
      </div>
      <div class="tab-pane " id="SRCol" role="tabpanel" aria-labelledby="SRC-tab">
          <ul>
              <li>H.V. Jagadish, University of Michigan</li>
              <li>Gerome University of Massachusetts Amherst</li>
              <li>Julia Stoyanovich, New York University</li>
          </ul>
      </div>
  </div>
  </div><!-- class="card" --></div></div>
      </div>
    <!-- Your code ends here -->
	<!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
</body>
</html>