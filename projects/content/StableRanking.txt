<div class="card">
<div class="card-header">Stable Rankings</div>
<div class="card-body">
<ul class="nav nav-tabs" id="StableR" role="tablist">
	<li class="nav-item"><a class="nav-link active" id="SRA-tab" data-toggle="tab" href="#SRAbstract" role="tab" aria-controls="home" aria-selected="true">Abstract</a></li>
	<li class="nav-item"><a class="nav-link" id="SRI-tab" data-toggle="tab" href="#SRIntro" role="tab" aria-controls="profile" aria-selected="false">Extended Description</a></li>
	<li class="nav-item"><a class="nav-link" id="SRP-tab" data-toggle="tab" href="#SRPubs" role="tab" aria-controls="contact" aria-selected="false">Publications</a></li>
	<li class="nav-item"><a class="nav-link" id="SRC-tab" data-toggle="tab" href="#SRCol" role="tab" aria-controls="contact" aria-selected="false">Collaborators</a></li>
</ul>
<div class="tab-content" id="SRContent">
	<div class="tab-pane active" id="SRAbstract" role="tabpanel" aria-labelledby="SRA-tab">
		Decision making is challenging when there is more than one crite-
		rion to consider. In such cases, it is common to assign a goodness
		score to each item as a weighted sum of its attribute values and
		rank them accordingly. Clearly, the ranking obtained depends on
		the weights used for this summation. Ideally, one would want the
		ranked order not to change if the weights are changed slightly. We
		call this property stability of the ranking. A consumer of a ranked
		list may trust the ranking more if it has high stability. A producer of
		a ranked list prefers to choose weights that result in a stable rank-
		ing, both to earn the trust of potential consumers and because a
		stable ranking is intrinsically likely to be more meaningful.
		In this paper, we develop a framework that can be used to assess
		the stability of a provided ranking and to obtain a stable ranking
		within an “acceptable” range of weight values (called “the region
		of interest”). We address the case where the user cares about the
		rank order of the entire set of items, and also the case where the user
		cares only about the top-k items. Using a geometric interpretation,
		we propose algorithms that produce stable rankings. In addition
		to theoretical analyses, we conduct extensive experiments on real
		datasets that validate our proposal.
	</div>
	<div class="tab-pane " id="SRIntro" role="tabpanel" aria-labelledby="SRI-tab">
		<p>
                <img src="assets/stability1.jpg" class="rounded float-right" style="padding-left: 10px;" />
                It is often useful to rank items in a dataset. It is straightforward to sort on a single attribute, but that is often not enough. When the items have more than one attribute on which they can be compared, it is challenging to place them in ranked order.
                Consider, for example, the problem of ranking computer science departments.  Various entities, such as U.S. News and World Report, Times Higher Education, and the National Research Council, produce such rankings.  
                These rankings are impactful, yet heavily criticized.
                Several of these rankings have deficiencies in the attributes they choose to measure and in their data collection methodology, not of relevance to our paper now.
                Our concern is that even if these deficiencies were addressed, we are compelled to obtain a single score/rank for a department by combining multiple objective measures, such as publications, citations, funding, and awards.  Different ways of combining values for these attributes can lead to very different rankings.  There are similar problems when we want to rank/seed sports teams, rank order cars or other products, 
                <a href="https://www.newyorker.com/magazine/2011/02/14/the-order-of-things" target="_blank">as Malcolm Gladwell has nicely described</a>.
        </p>
        <p>
                Differences in rank order can have significant consequences. 
                For example, a company may promote high-ranked employees and fire low-ranked employees.
                In university rankings, it is well-documented that the ranking formula has a significant effect on policies adopted by universities. In other words, it matters how we choose to combine values of multiple attributes into a scoring formula.
                Even when there is lack of consensus on a specific way to combine attributes, we should make sure that the method we use is robust: it should not be the case that small perturbations, such as small changes in parameter values, can change the rank order.
        </p>
        <p>
                In this project we address the following problem: <span class="bg-info text-warning">Assume that a linear combination of the attribute values is used for assigning a score to each item; then items are sorted to produce a final ranked order.  We want this ranking to be <i>stable</i> with respect to changes in the weights used in scoring</span>.  Given a particular ranked list of items, one question a consumer will ask is: how robust is the ranking?   If small changes in weights can change the ranked order, then there cannot be much confidence in the correctness of the ranking. 
        </p>
        <p>
                A given ranking of a set of items can be generated by many weight functions.  Because this set of functions is continuous, we can think of it as forming a region in the space of all possible weight functions.  We call a ranking of items stable if it is generated by a weight function that corresponds to a large region of this space.
        </p>
        <p>
                Stability is a natural concern for consumers of a ranked list (i.e. those who use the ranking to prioritize items and make decisions), who should be able to assess the magnitude of the region in the weight space that produces the observed ranking.  If this region is large, then the same ranked order would be obtained for many choices of weights, and the ranking is stable.  But if this region is small, then we know that only a few weight choices can produce the observed ranking.
                This may suggest that the ranking was engineered or <span class="bg-warning">cherry-picked</span> by the producer to obtain a specific outcome. 
        </p>
        <p>
                Data scientists often act as producers of ranked lists (i.e. they design weight functions that result in ranked lists), and desire to produce stable results.  Stability in a ranked output is an important aspect of algorithmic transparency, because it allows the producer to justify their ranking methodology, and to gain the trust of consumers.
                Of course, stability cannot be the only criterion in the choice of a ranking function: the result may be weights that seem ``unreasonable'' to the ranking producer.  To support the producer, we allow them to specify a range of reasonable weights, or an <i>acceptable region</i> in the space of functions, and help the producer find stable rankings within this region. 
        </p>
        <p>
                Our work is motivated by the lack of formal understanding of ranking stability and the consequent lack of tools for consumers and producers to assess this critical property of rankings.  We will show that stability hinges on complex geometric properties of rankings and weight functions.  We will provide a novel technique to identify stable rankings efficiently.
                Our technique does not stop at proposing just the single most stable choice, or even the <i>h</i> most stable choices for some pre-determined fixed value of <i>h</i>.
                Rather, it will continue to propose weight choices, and the corresponding rank ordering of items, beginning with the most stable in the specified region of interest, and continuing in decreasing order of stability, until the user finds one that is satisfactory.
                Alternatively, our technique can provide an overview of all the rankings that occupy a large portion in the acceptable region, and hence are stable, along with an indication of the fraction of the acceptable region occupied by each.  Thereby, the user can see at a glance what the stable options are, and also how dominant these are within the acceptable region.
                Let us motivate our techniques with an example:
        </p>
        <div class="card">
                <div class="card-header text-center">EXAMPLE</div>
                <div class="card-body">
                        <a href="http://csmetrics.org/" target="_blank">CSMetrics</a> ranks computer science research institutions based on the measured (<code>M</code>) and predicted (<code>P</code>) number of citations.
                        These values are appropriately scaled and used in a weighted scoring formula, with parameter <code>&alpha; &isin; [0,1]</code> that sets their relative importance (more details in the paper). 
                        CSMetrics includes a handful of companies in its ranking, but we focus on academic departments in this example.
                        As <code>&alpha;</code> ranges from 0 to 1, CSMetrics generates 336 distinct rankings of the top-100 departments.  Assuming (as a baseline) that each ranking is equally likely, we would expect an arbitrarily chosen ranking to occur 0.3% of the time, which we take to mean that it occupies 0.3% of the volume in the space of attributes and weights.
                        We formalize this as <i>stability of a ranking</i>.
                        <br>
                        Suppose that the ranking with <code>&alpha;=0.3</code> is released, placing Cornell (a consumer) at rank 11, just missing the top-10.
                        Cornell then checks the stability of the ranking and learns that it's value is 0.3%, matching that of the uniform baseline.  With this finding, Cornell asks CSMetrics to justify its choice of <code>&alpha;</code>.
                        <br>
                        CSMetrics (the producer) can respond to Cornell by further interrogating, and potentially revising the published ranking.
                        It first enumerates stable regions and finds that the most stable ranking indeed places Cornell at rank 10 (switching with the University of Toronto), and represents 2% of the volume --- an order of magnitude more than the reference ranking.
                        However, this stable ranking is very far from the default, placing more emphasis on measured citations with <code>&alpha;=0.608</code>.
                        If this is unsatisfactory, CSRankings can propose another ranking closer to the reference ranking, but with better stability.
                        Interestingly, Cornell also appears at the top-10 in the most stable ranking that is within 0.998 cosine similarity from the original scoring function. 
                </div>
        </div>
            <p>
                Besides studying the efficiency of our techniques, our empirical evaluation investigates the stability of real published rankings of computer science departments, soccer teams, and diamond retailers. 
                We show that existing rankings in these domains are often unstable and that favoring stability can sometimes have a significant impact on the rank of some items.
                For instance, <span class="bg-warning">our findings cast doubt on the validity of <a href="https://www.fifa.com/fifa-world-ranking/ranking-table/men/" target="_blank"> the FIFA rankings</a>
                which are used in making important decisions such as seeding the World Cup final draws.</span>
            </p>	
	</div>
	<div class="tab-pane " id="SRPubs" role="tabpanel" aria-labelledby="SRP-tab">
			Abolfazl Asudeh, HV Jagadish, Gerome Miklau, Julia Stoyanovich. <a target="_blank" href="https://dl.acm.org/citation.cfm?id=3308984">On obtaining stable rankings</a>. PVLDB, Vol. 12(3), pages 237--250, 2019, VLDB Endowment.
	</div>
	<div class="tab-pane " id="SRCol" role="tabpanel" aria-labelledby="SRC-tab">
		<ul>
			<li>H.V. Jagadish, University of Michigan</li>
			<li>Gerome University of Massachusetts Amherst</li>
			<li>Julia Stoyanovich, New York University</li>
		</ul>
	</div>
</div>
</div><!-- class="card" -->